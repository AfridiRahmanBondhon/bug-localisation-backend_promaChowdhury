{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os.path\n",
    "from collections import OrderedDict\n",
    "\n",
    "import javalang\n",
    "import pygments\n",
    "from pygments.lexers import JavaLexer\n",
    "from pygments.token import Token\n",
    "import re\n",
    "import inflection\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from assets import java_keywords, stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/promachowdhury/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/promachowdhury/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def src_parser(src_folder):\n",
    "        \"\"\"Parse source code directory of a program and collect\n",
    "        its java files.\n",
    "        \"\"\"\n",
    "\n",
    "       \n",
    "        src_addresses = glob.glob(str(src_folder) + '/**/*.java', recursive=True)\n",
    "\n",
    "        java_lexer = JavaLexer()\n",
    "\n",
    "        src_files = {}\n",
    "\n",
    "        for src_file in src_addresses:\n",
    "            with open(src_file, encoding='cp1256') as file:\n",
    "                src = file.read()\n",
    "\n",
    "            # Placeholder for different parts of a source file\n",
    "            comments = ''\n",
    "            class_names = []\n",
    "            attributes = []\n",
    "            method_names = []\n",
    "            variables = []\n",
    "\n",
    "            # Source parsing\n",
    "            parse_tree = None\n",
    "            try:\n",
    "                parse_tree = javalang.parse.parse(src)\n",
    "                for path, node in parse_tree.filter(javalang.tree.VariableDeclarator):\n",
    "                    if isinstance(path[-2], javalang.tree.FieldDeclaration):\n",
    "                        attributes.append(node.name)\n",
    "                    elif isinstance(path[-2], javalang.tree.VariableDeclaration):\n",
    "                        variables.append(node.name)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            \n",
    "            lexed_src = pygments.lex(src, java_lexer)\n",
    "\n",
    "            for i, token in enumerate(lexed_src):\n",
    "                if token[0] in Token.Comment:\n",
    "                 \n",
    "                    if i == 0 and token[0] is Token.Comment.Multiline:\n",
    "                        src = src[src.index(token[1]) + len(token[1]):]\n",
    "                        continue\n",
    "                    comments += token[1]\n",
    "                elif token[0] is Token.Name.Class:\n",
    "                    class_names.append(token[1])\n",
    "                elif token[0] is Token.Name.Function:\n",
    "                    method_names.append(token[1])\n",
    "\n",
    "           \n",
    "            if parse_tree and parse_tree.package:\n",
    "                package_name = parse_tree.package.name\n",
    "            else:\n",
    "                package_name = None\n",
    "\n",
    "                # If source file has package declaration\n",
    "            if package_name:\n",
    "                    src_id = (package_name + '.' +\n",
    "                              os.path.basename(src_file))\n",
    "            else:\n",
    "                    src_id = os.path.basename(src_file)\n",
    "\n",
    "            src_files[src_id] = [{\n",
    "                    'all_content':src,'comments': comments, 'class_names': class_names,'attributes': attributes,\n",
    "                    'method_names':method_names, 'variables':variables,\n",
    "                    'file_name':[os.path.basename(src_file).split('.')[0]],\n",
    "                    ' package_name': package_name\n",
    "                }]\n",
    "\n",
    "        return src_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from github import Github\n",
    "\n",
    "\n",
    "repo_name = \"jsoup\"\n",
    "repo_owner = 'jhy'\n",
    "github_token = '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'\n",
    "issue_number = 2079\n",
    "\n",
    "\n",
    "g = Github(github_token)\n",
    "\n",
    "\n",
    "repo = g.get_repo(f\"{repo_owner}/{repo_name}\")\n",
    "\n",
    "\n",
    "issue = repo.get_issue(issue_number)\n",
    "description = issue.body\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_parser('/Users/promachowdhury/Desktop/fast-projects/bug-localisation-backend/demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from github import Github\n",
    "\n",
    "\n",
    "repo_name = \"jsoup\"\n",
    "repo_owner = 'jhy'\n",
    "github_token = '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'\n",
    "issue_number = 2079\n",
    "\n",
    "\n",
    "g = Github(github_token)\n",
    "\n",
    "\n",
    "repo = g.get_repo(f\"{repo_owner}/{repo_name}\")\n",
    "\n",
    "\n",
    "issue = repo.get_issue(issue_number)\n",
    "description = issue.body\n",
    "summary = issue.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stack_traces(description):\n",
    "\n",
    "        regex = r\"at (.*?)\\((.*?)\\)\"\n",
    "\n",
    "        matches = re.findall(regex, description)\n",
    "\n",
    "        if matches:\n",
    "                for match in matches:\n",
    "                        print(\"Matched:\", match[1])\n",
    "        else:\n",
    "                print(\"No matches found.\")\n",
    "\n",
    "\n",
    "        #     # Filter actual stack traces from retrieved candidates\n",
    "        # st = [x for x in st_candid if any(s in x[1] for s in signs)]\n",
    "        # print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(entity):\n",
    "    t_summary = nltk.wordpunct_tokenize(entity)\n",
    "    return t_summary\n",
    "    # t_description = nltk.wordpunct_tokenize(description)\n",
    "\n",
    "    # return t_summary, t_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging(entity):\n",
    "    summ_tok = nltk.word_tokenize(entity)\n",
    "    sum_pos = nltk.pos_tag(summ_tok)\n",
    "\n",
    "    res = [\n",
    "                token for token, pos in sum_pos if \"NN\" in pos or \"VB\" in pos\n",
    "    ]\n",
    "    # desc_pos = nltk.pos_tag(t_description)\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_camelcase(tokens):\n",
    "\n",
    "        returning_tokens = tokens[:]\n",
    "\n",
    "        for token in tokens:\n",
    "            split_tokens = re.split(fr'[{string.punctuation}]+', token)\n",
    "\n",
    "            if len(split_tokens) > 1:\n",
    "                returning_tokens.remove(token)\n",
    "                for st in split_tokens:\n",
    "                    camel_split = inflection.underscore(st).split('_')\n",
    "                    if len(camel_split) > 1:\n",
    "                        returning_tokens.append(st)\n",
    "                        returning_tokens += camel_split\n",
    "                    else:\n",
    "                        returning_tokens.append(st)\n",
    "            else:\n",
    "                camel_split = inflection.underscore(token).split('_')\n",
    "                if len(camel_split) > 1:\n",
    "                    returning_tokens += camel_split\n",
    "\n",
    "        return returning_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_camelcase(entity):\n",
    "    return _split_camelcase(entity)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(entity):\n",
    "    return [\n",
    "                token for token in entity if token not in stop_words\n",
    "        ]\n",
    "def remove_java_keywords(entity):\n",
    "    return [\n",
    "                token for token in entity if token not in java_keywords\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(entity):\n",
    "        # Stemmer instance\n",
    "        stemmer = PorterStemmer()\n",
    "        res = dict(\n",
    "                zip(\n",
    "                    [\"stemmed\", \"unstemmed\"],\n",
    "                    [[stemmer.stem(token) for token in entity], entity],\n",
    "                )\n",
    "            )\n",
    "        return stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(entity):\n",
    "\n",
    "\n",
    "        punctnum_table = str.maketrans(\n",
    "            {c: None for c in string.punctuation + string.digits}\n",
    "        )\n",
    "        summary_punctnum_rem = [\n",
    "                token.translate(punctnum_table) for token in entity\n",
    "            ]\n",
    "        return summary_punctnum_rem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "        extract_stack_traces(description)\n",
    "        pos_desc = pos_tagging(description)\n",
    "        # token_desc = tokenize(pos_desc)\n",
    "        cc_desc = split_camelcase(pos_desc)\n",
    "        norm_desc = normalize(cc_desc)\n",
    "        rem_stop = remove_stopwords(norm_desc)\n",
    "        rem_java  = remove_java_keywords(rem_stop)\n",
    "        res = stem(rem_java)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traces_score(src_files, bug_reports):\n",
    "\n",
    "    all_file_names = set(s.exact_file_name for s in src_files.values())\n",
    "\n",
    "    all_scores = []\n",
    "    for report in bug_reports.values():\n",
    "\n",
    "        scores = []\n",
    "\n",
    "        stack_traces = report.stack_traces\n",
    "\n",
    "        final_st = []\n",
    "        for trace in stack_traces:\n",
    "            if trace[1] == 'Unknown Source':\n",
    "                final_st.append(\n",
    "                    (trace[0].split('.')[-2].split('$')[0], trace[0].strip()))\n",
    "            elif trace[1] != 'Native Method':\n",
    "                final_st.append(\n",
    "                    (trace[1].split('.')[0].replace(' ', ''), trace[0].strip()))\n",
    "\n",
    "        stack_traces = OrderedDict([(file, package) for file, package in final_st\n",
    "                                    if file in all_file_names])\n",
    "\n",
    "        for src in src_files.values():\n",
    "            file_name = src.exact_file_name\n",
    "\n",
    "            # If the source file has a package name\n",
    "            if src.package_name:\n",
    "                if file_name in stack_traces and src.package_name in stack_traces[file_name]:\n",
    "                    scores.append(\n",
    "                        1 / (list(stack_traces).index(file_name) + 1))\n",
    "\n",
    "                else:\n",
    "                    # If it isn't the exact source file based on it's package name\n",
    "                    scores.append(0)\n",
    "            # If it doesn't have a package name\n",
    "            elif file_name in stack_traces:\n",
    "                scores.append(1 / (list(stack_traces).index(file_name) + 1))\n",
    "            else:\n",
    "                scores.append(0)\n",
    "\n",
    "        all_scores.append(scores)\n",
    "\n",
    "    return all_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
